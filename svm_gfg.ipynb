{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Genre Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and functions\n",
    "\n",
    "from python_speech_features import mfcc\n",
    "import scipy.io.wavfile as wav\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from tempfile import TemporaryFile\n",
    "import os\n",
    "import pickle\n",
    "import random \n",
    "import operator\n",
    "\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(testSet, predictions):\n",
    "    correct = 0 \n",
    "    for x in range (len(testSet)):\n",
    "        if testSet[x][-1]==predictions[x]:\n",
    "            correct+=1\n",
    "    return 1.0*correct/len(testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/mnt/c/Users/nikhi/Desktop/final_project/genres\"\n",
    "f= open(\"my1.dat\" ,'wb')\n",
    "f2= open(\"my2.dat\", 'wb')\n",
    "i=0\n",
    "for folder in os.listdir(directory):\n",
    "    i+=1\n",
    "    if i==11 :\n",
    "        break   \n",
    "    for file in os.listdir(directory+\"/\"+folder):  \n",
    "        (rate,sig) = wav.read(directory+\"/\"+folder+\"/\"+file)\n",
    "        mfcc_feat = mfcc(sig,rate ,winlen=0.020, appendEnergy = False)\n",
    "        covariance = np.cov(np.matrix.transpose(mfcc_feat))\n",
    "        mean_matrix = mfcc_feat.mean(0)\n",
    "        feature = (mean_matrix , covariance , i)\n",
    "        pickle.dump(feature , f)\n",
    "        feature1 = (mean_matrix, covariance)\n",
    "        pickle.dump(feature1, f2)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = []\n",
    "# def loadDataset(filename , split , train , test):\n",
    "#     with open(\"my.dat\" , 'rb') as f:\n",
    "#         while True:\n",
    "#             try:\n",
    "#                 dataset.append(pickle.load(f))\n",
    "#             except EOFError:\n",
    "#                 f.close()\n",
    "#                 break  \n",
    "#     for x in range(len(dataset)):\n",
    "#         if random.random() <split :      \n",
    "#             train.append(dataset[x])\n",
    "#         else:\n",
    "#             test.append(dataset[x])  \n",
    "# trainingSet = []\n",
    "# testSet = []\n",
    "# loadDataset(\"my.dat\" , 0.8, trainingSet, testSet)\n",
    "\n",
    "\n",
    "def loadDataset(filename, dataset):\n",
    "    with open(filename , 'rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                dataset.append(pickle.load(f))\n",
    "            except EOFError:\n",
    "                f.close()\n",
    "                break   \n",
    "\n",
    "dataset1 = []      # Dataset with labels \n",
    "dataset2 = []      # Dataset without labels\n",
    "loadDataset(\"my1.dat\", dataset1)\n",
    "loadDataset(\"my2.dat\", dataset2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n",
      "1000\n",
      "13\n",
      "(1000, 182)\n",
      "[ 1  2  3  4  5  6  7  8  9 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhilbhave9/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "dataset1_np = np.array(dataset1) \n",
    "print(dataset1_np.shape)\n",
    "\n",
    "print(len(dataset2))\n",
    "\n",
    "# Current shape of dataset2: 1000 elements each containing 2 elements: #1: 13 elements #2: 13 elements each containing 13 elements\n",
    "print(len(dataset2[0][1][1]))\n",
    "\n",
    "# Declare a new numpy array for dataset2\n",
    "dataset2_np = np.empty((1000, 182))\n",
    "\n",
    "for i in range(len(dataset2)):\n",
    "    # Declare a temp array that we will append to the main np array\n",
    "    array_i = []\n",
    "    for j in range(len(dataset2[i][0])):\n",
    "        array_i.append(dataset2[i][0][j])\n",
    "    for k in range(len(dataset2[i][1])):\n",
    "        for k2 in range(len(dataset2[i][1][1])):\n",
    "            array_i.append(dataset2[i][1][k][k2])\n",
    "    array_np = np.array(array_i)\n",
    "    np.append(dataset2_np, array_np)\n",
    "\n",
    "\n",
    "print(dataset2_np.shape)\n",
    "\n",
    "# Preparing Target Data\n",
    "target = []\n",
    "for i in range(0, 1000):\n",
    "    target.append(dataset1_np[i][2])\n",
    "target_np = np.array(target)\n",
    "\n",
    "print(np.unique(target_np))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10 genres and each genre has a 100 songs each. Thus, there are a 1000 audio samples in total. \n",
    "Each song has 3 \"features\" associated with it associated with it. Each of these 3 features is represented using a tuple of varying sizes. \n",
    "- Feature #1: Mean_matrix -> A flat array of size 13\n",
    "- Feature #2: Covariance  -> A 2-D matrix of size 13 x 13\n",
    "- Feature #3: Label       -> An int representing the correct label for the audio sample \n",
    "\n",
    "However, we do not need the labels \"feature\" while fitting an SVM model to our data. We extract the labels from this dataset and add them to a \"target\" array. \n",
    "\n",
    "Now, we create another dataset, called dataset2, without labels as the third feature. In order to work with this second dataset and feed it to .fit method, we first need to flatten the data. Thus, we will now have, for each of the 1000 songs, one flat 1-D array of size 13 + (13 * 13) = 13 + 169 = 182. Thus, our final dataset2 shape should be (1000, 182)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the dataset \n",
    "Let's check for inconsistent values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2_np.shape[0]\n",
    "dataset2_np_copy = dataset2_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(0, dataset2_np.shape[0]):\n",
    "    for m in range(len(dataset2_np[n])):\n",
    "        if np.isnan(dataset2_np[n][m]):\n",
    "            dataset2_np[n][m] = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: \n",
      "(800, 182)\n",
      "Shape of X_test: \n",
      "(200, 182)\n",
      "Shape of y_train: \n",
      "(800,)\n",
      "Shape of y_test: \n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# Splice the dataset into X => 2 main features and Y => Correct labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset2_np, target_np, test_size=0.2,random_state=109) # 80% training and 20% test\n",
    "\n",
    "# Print shape of X_train \n",
    "print(\"Shape of X_train: \")\n",
    "print(X_train.shape)\n",
    "\n",
    "# Print shape of X_test \n",
    "print(\"Shape of X_test: \")\n",
    "print(X_test.shape)\n",
    "\n",
    "# Print shape of y_train \n",
    "print(\"Shape of y_train: \")\n",
    "print(y_train.shape)\n",
    "\n",
    "# Print shape of y_test \n",
    "print(\"Shape of y_test: \")\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting our data to an SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikhilbhave9/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:87: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/home/nikhilbhave9/.local/lib/python3.6/site-packages/numpy/core/_methods.py:192: RuntimeWarning: overflow encountered in reduce\n",
      "  arrmean = umr_sum(arr, axis, dtype, keepdims=True)\n",
      "/home/nikhilbhave9/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:87: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "#Create SVM Classifier\n",
    "classifier = svm.SVC(kernel='rbf')\n",
    "\n",
    "#Train the model using the training sets\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.09\n",
      "Precision: 0.09\n",
      "Recall: 0.09\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Model Precision: what percentage of positive tuples are labeled as such?\n",
    "print(\"Precision:\", metrics.precision_score(y_test, y_pred, average='micro'))\n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\", metrics.recall_score(y_test, y_pred,average=\"micro\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
